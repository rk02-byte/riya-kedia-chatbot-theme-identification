Speech and Language Processing.
Daniel Jurafsky & James H. Martin.
Copyright © 2024.
All
rights reserved.
Draft of August 20, 2024.
CHAPTER
10
Large Language Models
“How much do we know at any time? Much more, or so I believe, than we
know we know.”
Agatha Christie, The Moving Finger
Fluent speakers of a language bring an enormous amount of knowledge to bear dur-
ing comprehension and production. This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage. This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.
Estimates of the size of adult vocabularies vary widely both within and across
languages. For example, estimates of the vocabulary size of young adult speakers of
American English range from 30,000 to 100,000 depending on the resources used
to make the estimate and the deﬁnition of what it means to know a word. What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
caregivers and peers, usually well before the start of formal schooling. This active
vocabulary (usually on the order of 2000 words for young speakers) is extremely
limited compared to the size of the adult vocabulary, and is quite stable, with very
few additional words learned via casual conversation beyond this early stage. Obvi-
ously, this leaves a very large number of words to be acquired by other means.
A simple consequence of these facts is that children have to learn about 7 to 10
words a day, every single day, to arrive at observed vocabulary levels by the time they
are 20 years of age. And indeed empirical estimates of vocabulary growth in late el-
ementary through high school are consistent with this rate. How do children achieve
this rate of vocabulary growth? The bulk of this knowledge acquisition seems to
happen as a by-product of reading, as part of the rich processing and reasoning that
we perform when we read. Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate. But the mechanism behind this rate of learning must
be remarkable indeed, since at some points during learning the rate of vocabulary
growth exceeds the rate at which new words are appearing to the learner!
Such facts have motivated the distributional hypothesis of Chapter 6, which sug-
gests that aspects of meaning can be learned solely from the texts we encounter over
our lives, based on the complex association of words with the words they co-occur
with (and with the words that those words occur with). The distributional hypothe-
sis suggests both that we can acquire remarkable amounts of knowledge from text,
and that this knowledge can be brought to bear long after its initial acquisition. Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.
In this chapter we formalize this idea of pretraining—learning knowledge about
pretraining
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models. Large language models exhibit remark-
2
CHAPTER 10
•
LARGE LANGUAGE MODELS
able performance on all sorts of natural language tasks because of the knowledge
they learn in pretraining, and they will play a role throughout the rest of this book.
They have been especially transformative for tasks where we need to produce text,
like summarization, machine translation, question answering, or chatbots.
We’ll start by seeing how to apply the transformer of Chapter 9 to language
modeling, in a setting often called causal or autoregressive language models, in
which we iteratively predict words left-to-right from earlier words. We’ll ﬁrst in-
troduce training, seeing how language models are self-trained by iteratively being
taught to guess the next word in the text from the prior words.
We’ll then talk about the process of text generation. The application of LLMs
to generate text has vastly broadened the scope of NLP,. Text generation, code-
generation, and image-generation together constitute the important new area of gen-
erative AI. We’ll introduce speciﬁc algorithms for generating text from a language
generative AI
model, like greedy decoding and sampling. And we’ll see that almost any NLP
task can be modeled as word prediction in a large language model, if we think about
it in the right way. We’ll work through an example of using large language mod-
els to solve one classic NLP task of summarization (generating a short text that
summarizes some larger document).
10.1
Large Language Models with Transformers
The prior chapter introduced most of the components of a transformer in the domain
of language modeling: the transformer block including multi-head attention, the
language modeling head, and the positional encoding of the input. In the following
sections we’ll introduce the remaining aspects of the transformer LLM: sampling
and training. Before we do that, we use this section to talk about why and how we
apply transformer-based large language models to NLP tasks.
The tasks we will describe are all cases of conditional generation. Conditional
conditional
generation
generation is the task of generating text conditioned on an input piece of text. That
is, we give the LLM an input piece of text, generally called a prompt, and then have
the LLM continue generating text token by token, conditioned on the prompt. The
fact that transformers have such long contexts (many thousands of tokens) makes
them very powerful for conditional generation, because they can look back so far
into the prompting text.
Consider the simple task of text completion, illustrated in Fig. 10.1. Here a
language model is given a text preﬁx and is asked to generate a possible completion.
Note that as the generation process proceeds, the model has direct access to the
priming context as well as to all of its own subsequently generated outputs (at least
as much as ﬁts in the large context window). This ability to incorporate the entirety
of the earlier context and generated outputs at each time step is the key to the power
of large language models built from transformers.
So why should we care about predicting upcoming words or tokens? The in-
sight of large language modeling is that many practical NLP tasks can be cast as
word prediction, and that a powerful-enough language model can solve them with
a high degree of accuracy. For example, we can cast sentiment analysis as language
modeling by giving a language model a context like:
The sentiment of the sentence ‘‘I like Jackie Chan" is:
and comparing the following conditional probability of the words “positive” and the
10.1
•
LARGE LANGUAGE MODELS WITH TRANSFORMERS
3
Preﬁx Text
Completion Text
Encoder
Transformer
Blocks
Softmax
long
all
and
thanks
for
all
the
the
…
U
U
Unencoder layer
Language 
Modeling
Head
logits
So
E
i
+
E
i
+
E
i
+
E
i
+
E
i
+
E
i
+
E
i
+
…
Figure 10.1
Left-to-right (also called autoregressive) text completion with transformer-based large language
models. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.
word “negative” to see which is higher:
P(positive|The sentiment of the sentence ‘‘I like Jackie Chan" is:)
P(negative|The sentiment of the sentence ‘‘I like Jackie Chan" is:)
If the word “positive” is more probable, we say the sentiment of the sentence is
positive, otherwise we say the sentiment is negative.
We can also cast more complex tasks as word prediction. Consider question
answering, in which the system is given a question (for example a question with
a simple factual answer) and must give a textual answer; we introduce this task in
detail in Chapter 14. We can cast the task of question answering as word prediction
by giving a language model a question and a token like A: suggesting that an answer
should come next:
Q: Who wrote the book ‘‘The Origin of Species"?
A:
If we ask a language model to compute the probability distribution over possible
next words given this preﬁx:
P(w|Q: Who wrote the book ‘‘The Origin of Species"?
A:)
and look at which words w have high probabilities, we might expect to see that
Charles is very likely, and then if we choose Charles and continue and ask
P(w|Q: Who wrote the book ‘‘The Origin of Species"?
A: Charles)
we might now see that Darwin is the most probable token, and select it.
Conditional generation can even be used to accomplish tasks that must generate
longer responses. Consider the task of text summarization, which is to take a long
text
summarization
text, such as a full-length article, and produce an effective shorter summary of it. We
can cast summarization as language modeling by giving a large language model a
text, and follow the text by a token like tl;dr; this token is short for something like
4
CHAPTER 10
•
LARGE LANGUAGE MODELS
‘too long; didn’t read’ and in recent years people often use this token, especially in
informal work emails, when they are going to give a short summary. Since this token
is sufﬁciently frequent in language model training data, language models have seen
many texts in which the token occurs before a summary, and hence will interpret the
token as instructions to generate a summary. We can then do conditional generation:
give the language model this preﬁx, and then have it generate the following words,
one by one, and take the entire response as a summary. Fig. 10.2 shows an example
of a text and a human-produced summary from a widely-used summarization corpus
consisting of CNN and Daily Mirror news articles.
Original Article
The only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff
and offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur
Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough
for 10 to 15 snowballs, he says.
But not if you live in New England or surrounding states. “We will not ship snow to any states
in the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging
snow!”
His website and social media accounts claim to have ﬁlled more than 133 orders for snow – more
than 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a
record this winter for the snowiest month in its history. Most residents see the huge piles of snow
choking their yards and sidewalks as a nuisance, but Waring saw an opportunity.
According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-
eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He
joked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]
Summary
Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough
for 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.
Figure 10.2
Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus
(Hermann et al., 2015), (Nallapati et al., 2016).
If we take this full article and append the token tl;dr, we can use this as the con-
text to prime the generation process to produce a summary as illustrated in Fig. 10.3.
Again, what makes transformers able to succeed at this task (as compared, say, to
the primitive n-gram language model) is that attention can incorporate information
from the large context window, giving the model access to the original article as well
as to the newly generated text throughout the process.
Which words do we generate at each step? One simple way to generate words
is to always generate the most likely word given the context. Generating the most
likely word given the context is called greedy decoding. A greedy algorithm is one
greedy
decoding
that make a choice that is locally optimal, whether or not it will turn out to have
been the best choice with hindsight. Thus in greedy decoding, at each time step in
generation, the output yt is chosen by computing the probability for each possible
output (every word in the vocabulary) and then choosing the highest probability
word (the argmax):
ˆwt = argmaxw∈V P(w|w<t)
(10.1)
In practice, however, we don’t use greedy decoding with large language models.
A major problem with greedy decoding is that because the words it chooses are (by
deﬁnition) extremely predictable, the resulting text is generic and often quite repeti-
tive. Indeed, greedy decoding is so predictable that it is deterministic; if the context
10.2
•
SAMPLING FOR LLM GENERATION
5
Original Story
Generated Summary
…
idea
Kyle
was
born.
Kyle
Waring
Waring
only
The
…
will
Delimiter
will
U
U
U
tl;dr
LM Head
E
E
E
E
E
E
E
E
…
Figure 10.3
Summarization with large language models using the tl;dr token and context-based autore-
gressive generation.
is identical, and the probabilistic model is the same, greedy decoding will always re-
sult in generating exactly the same string. We’ll see in Chapter 13 that an extension
to greedy decoding called beam search works well in tasks like machine translation,
which are very constrained in that we are always generating a text in one language
conditioned on a very speciﬁc text in another language. In most other tasks, how-
ever, people prefer text which has been generated by more sophisticated methods,
called sampling methods, that introduce a bit more diversity into the generations.
We’ll see how to do that in the next few sections.
10.2
Sampling for LLM Generation
The core of the generation process for large language models is the task of choosing
the single word to generate next based on the context and based on the probabilities
that the model assigns to possible words. This task of choosing a word to generate
based on the model’s probabilities is called decoding. Decoding from a language
decoding
model in a left-to-right manner (or right-to-left for languages like Arabic in which
we read from right to left), and thus repeatedly choosing the next word conditioned
on our previous choices is called autoregressive generation or causal LM genera-
autoregressive
generation
tion.1 (As we’ll see, alternatives like the masked language models of Chapter 11 are
non-causal because they can predict words based on both past and future words).
The most common method for decoding in large language models is sampling.
Recall from Chapter 3 that sampling from a model’s distribution over words means
sampling
to choose random words according to their probability assigned by the model. That
is, we iteratively choose a word to generate according to its probability in context
1
Technically an autoregressive model predicts a value at time t based on a linear function of the values
at times t −1, t −2, and so on. Although language models are not linear (since they have many layers of
non-linearities), we loosely refer to this generation technique as autoregressive since the word generated
at each time step is conditioned on the word selected by the network from the previous step.
6
CHAPTER 10
•
LARGE LANGUAGE MODELS
as deﬁned by the model. Thus we are more likely to generate words that the model
thinks have a high probability in the context and less likely to generate words that
the model thinks have a low probability.
We saw back in Chapter 3 on page ?? how to generate text from a unigram lan-
guage model , by repeatedly randomly sampling words according to their probability
until we either reach a pre-determined length or select the end-of-sentence token. To
generate text from a trained transformer language model we’ll just generalize this
model a bit: at each step we’ll sample words according to their probability condi-
tioned on our previous choices, and we’ll use a transformer language model as the
probability model that tells us this probability.
We can formalize this algorithm for generating a sequence of wordsW = w1,w2,...,wN
until we hit the end-of-sequence token, using x ∼p(x) to mean ‘choose x by sam-
pling from the distribution p(x):
i←1
wi ∼p(w)
while wi != EOS
i←i + 1
wi ∼p(wi | w<i)
The algorithm above is called random sampling, and it turns out random sam-
random
sampling
pling doesn’t work well enough. The problem is that even though random sampling
is mostly going to generate sensible, high-probable words, there are many odd, low-
probability words in the tail of the distribution, and even though each one is low-
probability, if you add up all the rare words, they constitute a large enough portion
of the distribution that they get chosen often enough to result in generating weird
sentences. For this reason, instead of random sampling, we usually use sampling
methods that avoid generating the very unlikely words.
The sampling methods we introduce below each have parameters that enable
trading off two important factors in generation: quality and diversity. Methods
that emphasize the most probable words tend to produce generations that are rated
by people as more accurate, more coherent, and more factual, but also more boring
and more repetitive. Methods that give a bit more weight to the middle-probability
words tend to be more creative and more diverse, but less factual and more likely to
be incoherent or otherwise low-quality.
10.2.1
Top-k sampling
Top-k sampling is a simple generalization of greedy decoding. Instead of choosing
top-k sampling
the single most probable word to generate, we ﬁrst truncate the distribution to the
top k most likely words, renormalize to produce a legitimate probability distribution,
and then randomly sample from within these k words according to their renormalized
probabilities. More formally:
1. Choose in advance a number of words k
2. For each word in the vocabulary V, use the language model to compute the
likelihood of this word given the context p(wt|w<t)
3. Sort the words by their likelihood, and throw away any word that is not one of
the top k most probable words.
4. Renormalize the scores of the k words to be a legitimate probability distribu-
tion.
10.2
•
SAMPLING FOR LLM GENERATION
7
5. Randomly sample a word from within these remaining k most-probable words
according to its probability.
When k = 1, top-k sampling is identical to greedy decoding. Setting k to a larger
number than 1 leads us to sometimes select a word which is not necessarily the most
probable, but is still probable enough, and whose choice results in generating more
diverse but still high-enough-quality text.
10.2.2
Nucleus or top-p sampling
One problem with top-k sampling is that k is ﬁxed, but the shape of the probability
distribution over words differs in different contexts. If we set k = 10, sometimes
the top 10 words will be very likely and include most of the probability mass, but
other times the probability distribution will be ﬂatter and the top 10 words will only
include a small part of the probability mass.
An alternative, called top-p sampling or nucleus sampling (Holtzman et al.,
top-p sampling
2020), is to keep not the top k words, but the top p percent of the probability mass.
The goal is the same; to truncate the distribution to remove the very unlikely words.
But by measuring probability rather than the number of words, the hope is that the
measure will be more robust in very different contexts, dynamically increasing and
decreasing the pool of word candidates.
Given a distribution P(wt|w<t), the top-p vocabulary V (p) is the smallest set of
words such that
X
w∈V (p)
P(w|w<t) ≥p.
(10.2)
10.2.3
Temperature sampling
In temperature sampling, we don’t truncate the distribution, but instead reshape
temperature
sampling
it. The intuition for temperature sampling comes from thermodynamics, where a
system at a high temperature is very ﬂexible and can explore many possible states,
while a system at a lower temperature is likely to explore a subset of lower energy
(better) states. In low-temperature sampling, we smoothly increase the probability
of the most probable words and decrease the probability of the rare words.
We implement this intuition by simply dividing the logit by a temperature param-
eter τ before we normalize it by passing it through the softmax. In low-temperature
sampling, τ ∈(0,1]. Thus instead of computing the probability distribution over the
vocabulary directly from the logit as in the following (repeated from (??)):
y = softmax(u)
(10.3)
we instead ﬁrst divide the logits by τ, computing the probability vector y as
y = softmax(u/τ)
(10.4)
Why does this work? When τ is close to 1 the distribution doesn’t change much.
But the lower τ is, the larger the scores being passed to the softmax (dividing by a
smaller fraction τ ≤1 results in making each score larger). Recall that one of the
useful properties of a softmax is that it tends to push high values toward 1 and low
values toward 0. Thus when larger numbers are passed to a softmax the result is
a distribution with increased probabilities of the most high-probability words and
decreased probabilities of the low probability words, making the distribution more
greedy. As τ approaches 0 the probability of the most likely word approaches 1.
8
CHAPTER 10
•
LARGE LANGUAGE MODELS
Note, by the way, that there can be other situations where we may want to do
something quite different and ﬂatten the word probability distribution instead of
making it greedy. Temperature sampling can help with this situation too, in this case
high-temperature sampling, in which case we use τ > 1.
10.3
Pretraining Large Language Models
How do we teach a transformer to be a language model? What is the algorithm and
what data do we train on?
10.3.1
Self-supervised training algorithm
To train a transformer as a language model, we use the same self-supervision (or
self-supervision
self-training) algorithm we saw in Section ??: we take a corpus of text as training
material and at each time step t ask the model to predict the next word. We call
such a model self-supervised because we don’t have to add any special gold labels
to the data; the natural sequence of words is its own supervision! We simply train the
model to minimize the error in predicting the true next word in the training sequence,
using cross-entropy as the loss function.
Recall that the cross-entropy loss measures the difference between a predicted
probability distribution and the correct distribution.
LCE = −
X
w∈V
yt[w]log ˆyt[w]
(10.5)
In the case of language modeling, the correct distribution yt comes from knowing the
next word. This is represented as a one-hot vector corresponding to the vocabulary
where the entry for the actual next word is 1, and all the other entries are 0. Thus,
the cross-entropy loss for language modeling is determined by the probability the
model assigns to the correct next word (all other words get multiplied by zero). So
at time t the CE loss in (10.5) can be simpliﬁed as the negative log probability the
model assigns to the next word in the training sequence.
LCE(ˆyt,yt) = −log ˆyt[wt+1]
(10.6)
Thus at each word position t of the input, the model takes as input the correct se-
quence of tokens w1:t, and uses them to compute a probability distribution over
possible next words so as to compute the model’s loss for the next token wt+1. Then
we move to the next word, we ignore what the model predicted for the next word
and instead use the correct sequence of tokens w1:t+1 to estimate the probability of
token wt+2. This idea that we always give the model the correct history sequence to
predict the next word (rather than feeding the model its best case from the previous
time step) is called teacher forcing.
teacher forcing
Fig. 10.4 illustrates the general training approach. At each step, given all the
preceding words, the ﬁnal transformer layer produces an output distribution over
the entire vocabulary. During training, the probability assigned to the correct word
is used to calculate the cross-entropy loss for each item in the sequence. The loss
for a training sequence is the average cross-entropy loss over the entire sequence.
The weights in the network are adjusted to minimize the average CE loss over the
training sequence via gradient descent.
10.3
•
PRETRAINING LARGE LANGUAGE MODELS
9
long
and
thanks
for
Next token
all
Loss
…
=
−log yand
Stacked
Transformer
Blocks
So
long
and
thanks
for
…
…
…
U
Input tokens
x1
x2
Language
Modeling
Head
x3
x4
x5
Input
Encoding
E
1
+
E
2
+
E
3
+
E
4
+
E
5
+
…
…
…
…
…
U
U
U
U
…
logits
logits
logits
logits
logits
…
−log ythanks
Figure 10.4
Training a transformer as a language model.
Note the key difference between this ﬁgure and the earlier RNN-based version
shown in Fig. ??. There the calculation of the outputs and the losses at each step was
inherently serial given the recurrence in the calculation of the hidden states. With
transformers, each training item can be processed in parallel since the output for
each element in the sequence is computed separately.
Large models are generally trained by ﬁlling the full context window (for exam-
ple 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter
than this, multiple documents are packed into the window with a special end-of-text
token between them. The batch size for gradient descent is usually quite large (the
largest GPT-3 model uses a batch size of 3.2 million tokens).
10.3.2
Training corpora for large language models
Large language models are mainly trained on text scraped from the web, augmented
by more carefully curated data. Because these training corpora are so large, they are
likely to contain many natural examples that can be helpful for NLP tasks, such as
question and answer pairs (for example from FAQ lists), translations of sentences
between various languages, documents together with their summaries, and so on.
Web text is usually taken from corpora of automatically-crawled web pages like
the common crawl, a series of snapshots of the entire web produced by the non-
common crawl
proﬁt Common Crawl (https://commoncrawl.org/) that each have billions of
webpages. Various versions of common crawl data exist, such as the Colossal Clean
Crawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English
that is ﬁltered in various ways (deduplicated, removing non-natural language like
code, sentences with offensive words from a blocklist). This C4 corpus seems to
consist in large part of patent text documents, Wikipedia, and news sites (Dodge
et al., 2021).
Wikipedia plays a role in lots of language model training, as do corpora of books.
The Pile (Gao et al., 2020) is an 825 GB English text corpus that is constructed by
The Pile
publicly released code, containing again a large amount of text scraped from the web
10
CHAPTER 10
•
LARGE LANGUAGE MODELS
as well as books and Wikipedia; Fig. 10.5 shows its composition. Dolma is a larger
open corpus of English, created with public tools, containing three trillion tokens,
which similarly consists of web text, academic papers, code, books, encyclopedic
materials, and social media (Soldaini et al., 2024).
Figure 10.5
The Pile corpus, showing the size of different components, color coded as
academic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext in-
cluding a subset of the common crawl as well as Wikipedia), prose (a large corpus of books),
dialogue (including movie subtitles and chat data), and misc.. Figure from Gao et al. (2020).
Filtering for quality and safety
Pretraining data drawn from the web is ﬁltered
for both quality and safety. Quality ﬁlters are classiﬁers that assign a score to each
document. Quality is of course subjective, so different quality ﬁlters are trained
in different ways, but often to value high-quality reference corpora like Wikipedia,
books, and particular websites and to avoid websites with lots of PII (Personal Iden-
PII
tiﬁable Information) or adult content. Filters also remove boilerplate text which is
very frequent on the web. Another kind of quality ﬁltering is deduplication, which
can be done at various levels, so as to remove duplicate documents, duplicate web
pages, or duplicate text. Quality ﬁltering generally improves language model per-
formance (Longpre et al., 2024b; Llama Team, 2024).
Safety ﬁltering is again a subjective decision, and often includes toxicity detec-
tion based on running off-the-shelf toxicity classiﬁers. This can have mixed results.
One problem is that current toxicity classiﬁers mistakenly ﬂag non-toxic data if it
is generated by speakers of minority dialects like African American English (Xu
et al., 2021). Another problem is that models trained on toxicity-ﬁltered data, while
somewhat less toxic, are also worse at detecting toxicity themselves (Longpre et al.,
2024b). These issues make the question of how to do better safety ﬁltering an im-
portant open problem.
Using large datasets scraped from the web to train language models poses ethical
and legal questions:
Copyright: Much of the text in these large datasets (like the collections of ﬁc-
tion and non-ﬁction books) is copyrighted. In some countries, like the United
States, the fair use doctrine may allow copyrighted content to be used for
transformative uses, but it’s not clear if that remains true if the language mod-
els are used to generate text that competes with the market for the text they
10.3
•
PRETRAINING LARGE LANGUAGE MODELS
11
are trained on (Henderson et al., 2023).
Data consent: Owners of websites can indicate that they don’t want their sites
to be crawled by web crawlers (either via a robots.txt ﬁle, or via Terms of
Service). Recently there has been a sharp increase in the number of web-
sites that have indicated that they don’t want large language model builders
crawling their sites for training data (Longpre et al., 2024a). Because it’s not
clear what legal status these indications have in different countries, or whether
these restrictions are retroactive, what effect this will have on large pretraining
datasets is unclear.
Privacy: Large web datasets also have privacy issues since they contain private
information like phone numbers and IP addresses. While ﬁlters are used to try
to remove websites likely to contain large amounts of personal information,
such ﬁltering isn’t sufﬁcient.
10.3.3
Finetuning
Although the enormous pretraining data for a large language model includes text
from many domains, it’s often the case that we want to apply it in a new domain or
task that might not have appeared sufﬁciently in the pre-training data. For example,
we might want a language model that’s specialized to legal or medical text. Or we
might have a multilingual language model that knows many languages but might
beneﬁt from some more data in our particular language of interest. Or we want a
language model that is specialized to a particular task.
In such cases, we can simply continue training the model on relevant data from
the new domain or language (Gururangan et al., 2020). This process of taking a fully
pretrained model and running additional training passes on some new data is called
ﬁnetuning. Fig. 10.6 sketches the paradigm.
ﬁnetuning
Fine-
tuning 
Data
Pretraining Data
Pretraining
…
…
…
Fine-tuning
…
…
…
Pretrained LM
Fine-tuned LM
Figure 10.6
Pretraining and ﬁnetuning. A pre-trained model can be ﬁnetuned to a par-
ticular domain, dataset, or task. There are many different ways to ﬁnetune, depending on
exactly which parameters are updated from the ﬁnetuning data: all the parameters, some of
the parameters, or only the parameters of speciﬁc extra circuitry.
We’ll introduce four related kinds of ﬁnetuning in this chapter and the two fol-
lowing chapters. In all four cases, ﬁnetuning means the process of taking a pre-
trained model and further adapting some or all of its parameters to some new data.
But they differ on exactly which parameters get updated.
In the ﬁrst kind of ﬁnetuning we retrain all the parameters of the model on this
new data, using the same method (word prediction) and loss function (cross-entropy
loss) as for pretraining. In a sense it’s as if the new data were at the tail end of
12
CHAPTER 10
•
LARGE LANGUAGE MODELS
the pretraining data, and so you’ll sometimes see this method called continued pre-
training.
continued
pretraining
Retraining all the parameters of the model is very slow and expensive when the
language model is huge. So instead we can freeze some of the parameters (i.e., leave
freeze
them unchanged from their pretrained value) and train only a subset of parameters
on the new data. In Section 10.5.3 we’ll describe this second variety of ﬁnetun-
ing, called parameter-efﬁcient ﬁnetuning, or PEFT. because we efﬁciently select
speciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrained
values.
In Chapter 11 we’ll introduce a third kind of ﬁnetuning, also parameter-efﬁcient.
In this version, the goal is to use a language model as a kind of classiﬁer or labeler
for a speciﬁc task. For example we might train the model to be a sentiment classiﬁer.
We do this by adding extra neural circuitry (an extra head) after the top layer of the
model. This classiﬁcation head takes as input some of the top layer embeddings of
the transformer and produces as output a classiﬁcation. In this method, most com-
monly used with masked language models like BERT, we freeze the entire pretrained
model and only train the classiﬁcation head on some new data, usually labeled with
some class that we want to predict.
Finally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-
cial component of the largest language models: supervised ﬁnetuning or SFT. SFT
is often used for instruction ﬁnetuning, in which we want a pretrained language
model to learn to follow text instructions, for example to answer questions or follow
a command to write something. Here we create a dataset of prompts and desired
responses (for example questions and their answers, or commands and their ful-
ﬁllments), and we train the language model using the normal cross-entropy loss to
predict each token in the instruction prompt iteratively, essentially training it to pro-
duce the desired response from the command in the prompt. It’s called supervised
because unlike in pretraining, where we just take any data and predict the words in
it, we build the special ﬁnetuning dataset by hand, creating supervised responses to
each command.
Often everything that happens after pretraining is lumped together as post-training;
we’ll discuss the various parts of post-training in Chapter 12.
10.4
Evaluating Large Language Models
Perplexity
As we ﬁrst saw in Chapter 3, one way to evaluate language models is
to measure how well they predict unseen text. Intuitively, good models are those that
assign higher probabilities to unseen data (are less surprised when encountering the
new words).
We instantiate this intuition by using perplexity to measure the quality of a
perplexity
language model. Recall from page ?? that the perplexity of a model θ on an unseen
test set is the inverse probability that θ assigns to the test set, normalized by the test
set length. For a test set of n tokens w1:n, the perplexity is
Perplexityθ(w1:n) = Pθ(w1:n)−1
n
=
n
s
1
Pθ(w1:n)
(10.7)
To visualize how perplexity can be computed as a function of the probabilities the
10.4
•
EVALUATING LARGE LANGUAGE MODELS
13
LM computes for each new word, we can use the chain rule to expand the computa-
tion of probability of the test set:
Perplexityθ(w1:n) =
n
v
u
u
t
n
Y
i=1
1
Pθ(wi|w<i)
(10.8)
Note that because of the inverse in Eq. 10.7, the higher the probability of the word
sequence, the lower the perplexity. Thus the the lower the perplexity of a model on
the data, the better the model. Minimizing perplexity is equivalent to maximizing
the test set probability according to the language model.
One caveat: because perplexity depends on the length of a text, it is very sensitive
to differences in the tokenization algorithm. That means that it’s hard to exactly
compare perplexities produced by two language models if they have very different
tokenizers. For this reason perplexity is best used when comparing language models
that use the same tokenizer.
Other factors
While the predictive accuracy of a language model, as measured by
perplexity, is a very useful metric, we also care about different kinds of accuracy, for
the downstream tasks we apply our language model to. For each task like machine
translation, summarization, question answering, speech recognition, and dialogue,
we can measure the accuracy at those tasks. Future chapters will introduce task-
speciﬁc metrics that allow us to evaluate how accuracy or correct language models
are at these downstream tasks.
But when evaluating models we also care about factors besides any of these
kinds of accuracy (Dodge et al., 2019; Ethayarajh and Jurafsky, 2020). For example,
we often care about how a big a model is, and how long it takes to train or do
inference. This can matter because we have constraints on time either for training
or at inference. Or we may have constraints on memory, since the GPUs we run
our models on have ﬁxed memory sizes. Big models also use more energy, and we
prefer models that use less energy, both to reduce the environmental impact of the
model and to reduce the ﬁnancial cost of building or deploying it. We can target
our evaluation to these factors by measuring performance normalized to a giving
compute or memory budget. We can also directly measure the energy usage of our
model in kWh or in kilograms of CO2 emitted (Strubell et al., 2019; Henderson
et al., 2020; Liang et al., 2023).
Another feature that a language model evaluation can measure is fairness. We
know that language models are biased, exhibiting gendered and racial stereotypes,
or decreased performance for language from or about certain demographics groups.
There are language model evaluation benchmarks that measure the strength of these
biases, such as StereoSet (Nadeem et al., 2021), RealToxicityPrompts (Gehman
et al., 2020), and BBQ (Parrish et al., 2022) among many others. We also want
language models whose performance is equally fair to different groups. For exam-
ple, we could chose an evaluation that is fair in a Rawlsian sense by maximizing the
welfare of the worst-off group (Rawls, 2001; Hashimoto et al., 2018; Sagawa et al.,
2020).
Finally, there are many kinds of leaderboards like Dynabench (Kiela et al., 2021)
and general evaluation protocols like HELM (Liang et al., 2023); we will return to
these in later chapters when we introduce evaluation metrics for speciﬁc tasks like
question answering and information retrieval.
14
CHAPTER 10
•
LARGE LANGUAGE MODELS
10.5
Dealing with Scale
Large language models are large. For example the Llama 3.1 405B Instruct model
from Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,
128 attention heads) and was trained on 15.6 terabytes of text tokens (Llama Team,
2024), using a vocabulary of 128K tokens. So there is a lot of research on un-
derstanding how LLMs scale, and especially how to implement them given limited
resources. In the next few sections we discuss how to think about scale (the concept
of scaling laws), and important techniques for getting language models to work
efﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.
10.5.1
Scaling laws
The performance of large language models has shown to be mainly determined by
3 factors: model size (the number of parameters not counting embeddings), dataset
size (the amount of training data), and the amount of compute used for training. That
is, we can improve a model by adding parameters (adding more layers or having
wider contexts or both), by training on more data, or by training for more iterations.
The relationships between these factors and performance are known as scaling
laws. Roughly speaking, the performance of a large language model (the loss) scales
scaling laws
as a power-law with each of these three properties of model training.
For example, Kaplan et al. (2020) found the following three relationships for
loss L as a function of the number of non-embedding parameters N, the dataset size
D, and the compute budget C, for models training with limited parameters, dataset,
or compute budget, if in each case the other two properties are held constant:
L(N) =
Nc
N
αN
(10.9)
L(D) =
Dc
D
αD
(10.10)
L(C) =
Cc
C
αC
(10.11)
The number of (non-embedding) parameters N can be roughly computed as fol-
lows (ignoring biases, and with d as the input and output dimensionality of the
model, dattn as the self-attention layer size, and dff the size of the feedforward layer):
N ≈2 d nlayer(2 dattn +dff)
≈12 nlayer d2
(10.12)
(assuming dattn = dff/4 = d)
Thus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 ×
122882 ≈175 billion parameters.
The values of Nc, Dc, Cc, αN, αD, and αC depend on the exact transformer
architecture, tokenization, and vocabulary size, so rather than all the precise values,
scaling laws focus on the relationship with loss.2
Scaling laws can be useful in deciding how to train a model to a particular per-
formance, for example by looking at early in the training curve, or performance with
2
For the initial experiment in Kaplan et al. (2020) the precise values were αN = 0.076, Nc = 8.8 ×1013
(parameters), αD = 0.095, Dc = 5.4 ×1013 (tokens), αC = 0.050, Cc = 3.1 ×108 (petaﬂop-days).
10.5
•
DEALING WITH SCALE
15
smaller amounts of data, to predict what the loss would be if we were to add more
data or increase model size. Other aspects of scaling laws can also tell us how much
data we need to add when scaling up a model.
10.5.2
KV Cache
We saw in Fig. ?? and in Eq. ?? (repeated below) how the attention vector can be
very efﬁciently computed in parallel for training, via two matrix multiplications:
A = softmax
QK⊺
√dk

V
(10.13)
Unfortunately we can’t do quite the same efﬁcient computation in inference as
in training. That’s because at inference time, we iteratively generate the next tokens
one at a time. For a new token that we have just generated, call it xi, we need to
compute its query, key, and values by multiplying by WQ, WK, and WV respec-
tively. But it would be a waste of computation time to recompute the key and value
vectors for all the prior tokens x<i; at prior steps we already computed these key
and value vectors! So instead of recomputing these, whenever we compute the key
and value vectors we store them in memory in the KV cache, and then we can just
KV cache
grab them from the cache when we need them. Fig. 10.7 modiﬁes Fig. ?? to show
the computation that takes place for a single new token, showing which values we
can take from the cache rather than recompute.
q4
k1
k2
k4
Q
KT
QKT
v1
v2
v3
v4
V
q4•k1 q4•k2 q4•k3 q4•k4
x
=
=
x
a4
A
1 x dk
dk x N
1 x N
N x dv
1 x dv
k3
Figure 10.7
Parts of the attention computation (extracted from Fig. ??) showing, in black,
the vectors that can be stored in the cache rather than recomputed when computing the atten-
tion score for the 4th token.
10.5.3
Parameter Efﬁcient Fine Tuning
As we mentioned above, it’s very common to take a language model and give it more
information about a new domain by ﬁnetuning it (continuing to train it to predict
upcoming words) on some additional data.
Fine-tuning can be very difﬁcult with very large language models, because there
are enormous numbers of parameters to train; each pass of batch gradient descent
has to backpropagate through many many huge layers. This makes ﬁnetuning huge
language models extremely expensive in processing power, in memory, and in time.
For this reason, there are alternative methods that allow a model to be ﬁnetuned
without changing all the parameters. Such methods are called parameter-efﬁcient
ﬁne tuning or sometimes PEFT, because we efﬁciently select a subset of parameters
parameter-
efﬁcient ﬁne
tuning
PEFT
to update when ﬁnetuning. For example we freeze some of the parameters (don’t
change them), and only update some particular subset of parameters.
16
CHAPTER 10
•
LARGE LANGUAGE MODELS
Here we describe one such model, called LoRA, for Low-Rank Adaptation. The
LoRA
intuition of LoRA is that transformers have many dense layers which perform matrix
multiplication (for example the WQ, WK, WV, WO layers in the attention computa-
tion). Instead of updating these layers during ﬁnetuning, with LoRA we freeze these
layers and instead update a low-rank approximation that has fewer parameters.
Consider a matrix W of dimensionality [N ×d] that needs to be updated during
ﬁnetuning via gradient descent. Normally this matrix would get updates ∆W of
dimensionality [N ×d], for updating the N ×d parameters after gradient descent. In
LoRA, we freeze W and update instead a low-rank decomposition of W. We create
two matrices A and B, where A has size [N ×r] and B has size [r×d], and we choose
r to be quite small, r << min(d,N). During ﬁnetuning we update A and B instead
of W. That is, we replace W + ∆W with W + BA. Fig. 10.8 shows the intuition.
For replacing the forward pass h = xW, the new forward pass is instead:
h = xW +xAB
(10.14)
h
Pretrained 
Weights
W
d
k
r
k
A
B
r
x
d
1
1
k
d
×
Figure 10.8
The intuition of LoRA. We freeze W to its pretrained values, and instead ﬁne-
tune by training a pair of matrices A and B, updating those instead of W, and just sum W and
the updated AB.
LoRA has a number of advantages. It dramatically reduces hardware require-
ments, since gradients don’t have to be calculated for most parameters. The weight
updates can be simply added in to the pretrained weights, since BA is the same size
as W). That means it doesn’t add any time during inference. And it also means it’s
possible to build LoRA modules for different domains and just swap them in and
out by adding them in or subtracting them from W.
In its original version LoRA was applied just to the matrices in the attention
computation (the WQ, WK, WV, and WO layers). Many variants of LoRA exist.
10.6
•
POTENTIAL HARMS FROM LANGUAGE MODELS
17
10.6
Potential Harms from Language Models
Large pretrained neural language models exhibit many of the potential harms dis-
cussed in Chapter 4 and Chapter 6. Many of these harms become realized when
pretrained language models are used for any downstream task, particularly those
involving text generation, whether question answering, machine translation, or in
assistive technologies like writing aids or web search query completion, or predic-
tive typing for email (Olteanu et al., 2020).
For example, language models are prone to saying things that are false, a prob-
lem called hallucination. Language models are trained to generate text that is pre-
hallucination
dictable and coherent, but the training algorithms we have seen so far don’t have
any way to enforce that the text that is generated is correct or true. This causes
enormous problems for any application where the facts matter! We’ll return to this
issue in Chapter 14 where we introduce proposed mitigation methods like retrieval
augmented generation.
A second source of harm is that language models can generate toxic language.
toxic language
Gehman et al. (2020) show that even completely non-toxic prompts can lead large
language models to output hate speech and abuse their users. Language models also
generate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020;
Sheng et al., 2019) about many demographic groups.
One source of biases is the training data. Gehman et al. (2020) shows that large
language model training datasets include toxic text scraped from banned sites. There
are other biases than toxicity: the training data is disproportionately generated by
authors from the US and from developed countries. Such biased population samples
likely skew the resulting generation toward the perspectives or topics of this group
alone. Furthermore, language models can amplify demographic and other biases in
training data, just as we saw for embedding models in Chapter 6.
Datasets can be another source of harms. We already saw in Section 10.3.2
that using pretraining corpora scraped from the web can lead to harms related to
copyright and data consent. We also mentioned that pretraining data can tend to
have private information like phone numbers and addresses. This is problematic
because large language models can leak information from their training data. That
is, an adversary can extract training-data text from a language model such as a per-
son’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2021).
This becomes even more problematic when large language models are trained on
extremely sensitive private datasets such as electronic health records.
Language models can also be used by malicious actors for generating text for
misinformation, phishing, or other socially harmful activities (Brown et al., 2020).
McGufﬁe and Newhouse (2020) show how large language models generate text that
emulates online extremists, with the risk of amplifying extremist movements and
their attempt to radicalize and recruit.
Finding ways to mitigate all these harms is an important current research area in
NLP. At the very least, carefully analyzing the data used to pretrain large language
models is important as a way of understanding issues of toxicity, bias, privacy, and
fair use, making it extremely important that language models include datasheets
(page ??) or model cards (page ??) giving full replicable information on the cor-
pora used to train them. Open-source models can specify their exact training data.
Requirements that models are transparent in such ways is also in the process of being
incorporated into the regulations of various national governments.
18
CHAPTER 10
•
LARGE LANGUAGE MODELS
10.7
Summary
This chapter has introduced the large language model, and how it can be built out of
the transformer. Here’s a summary of the main points that we covered:
• Many NLP tasks—such as question answering, summarization, sentiment,
and machine translation—can be cast as tasks of word prediction and hence
addressed with Large language models.
• Large language models are generally pretrained on large datasets of 100s of
billions of words generally scraped from the web.
• These datasets need to be ﬁltered for quality and balanced for domains by
upsampling and downsampling. Addressing some problems with pretraining
data, like toxicity, are open research problems.
• The choice of which word to generate in large language models is generally
done by using a sampling algorithm.
• Language models are evaluated by perplexity but there are also evaluations
of accuracy downstream tasks, and ways to measure other factors like fairness
and energy use.
• There are various computational tricks for making large language models
more efﬁcient, such as the CV cache and parameter-efﬁcient ﬁnetuning.
• Because of their ability to be used in so many ways, language models also
have the potential to cause harms. Some harms include hallucinations, bias,
stereotypes, misinformation and propaganda, and violations of privacy and
copyright.
Bibliographical and Historical Notes
As we discussed in Chapter 3, the earliest language models were the n-gram lan-
guage models developed (roughly simultaneously and independently) by Fred Je-
linek and colleagues at the IBM Thomas J. Watson Research Center, and James
Baker at CMU. It was the Jelinek and the IBM team who ﬁrst coined the term lan-
guage model to mean a model of the way any kind of linguistic property (grammar,
semantics, discourse, speaker characteristics), inﬂuenced word sequence probabil-
ities (Jelinek et al., 1975). They contrasted the language model with the acoustic
model which captured acoustic/phonetic characteristics of phone sequences.
N-gram language models were very widely used over the next 30 years and more,
across a wide variety of NLP tasks like speech recognition and machine translations,
often as one of multiple components of the model. The contexts for these n-gram
models grew longer, with 5-gram models used quite commonly by very efﬁcient LM
toolkits (Stolcke, 2002; Heaﬁeld, 2011).
The roots of the neural language model lie in multiple places. One was the
application in the 1990s, again in Jelinek’s group at IBM Research, of discrimi-
native classiﬁers to language models. Roni Rosenfeld in his dissertation (Rosen-
feld, 1992) ﬁrst applied logistic regression (under the name maximum entropy or
maxent models) to language modeling in that IBM lab, and published a more fully
formed version in Rosenfeld (1996). His model integrated various sorts of infor-
mation in a logistic regression predictor, including n-gram information along with
BIBLIOGRAPHICAL AND HISTORICAL NOTES
19
other features from the context, including distant n-grams and pairs of associated
words called trigger pairs. Rosenfeld’s model preﬁgured modern language models
by being a statistical word predictor trained in a self-supervised manner simply by
learning to predict upcoming words in a corpus.
Another was the ﬁrst use of pretrained embeddings to model word meaning in the
LSA/LSI models (Deerwester et al., 1988). Recall from the history section of Chap-
ter 6 that in LSA (latent semantic analysis) a term-document matrix was trained on a
corpus and then singular value decomposition was applied and the ﬁrst 300 dimen-
sions were used as a vector embedding to represent words. Landauer et al. (1997)
ﬁrst used the word “embedding”. In addition to their development of the idea of pre-
training and of embeddings, the LSA community also developed ways to combine
LSA embeddings with n-grams in an integrated language model (Bellegarda, 1997;
Coccaro and Jurafsky, 1998).
In a very inﬂuential series of papers developing the idea of neural language
models, (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Ben-
gio and colleagues drew on the central ideas of both these lines of self-supervised
language modeling work, (the discriminatively trained word predictor, and the pre-
trained embeddings). Like the maxent models of Rosenfeld, Bengio’s model used
the next word in running text as its supervision signal. Like the LSA models, Ben-
gio’s model learned an embedding, but unlike the LSA models did it as part of the
process of language modeling. The Bengio et al. (2003) model was a neural lan-
guage model: a neural network that learned to predict the next word from prior
words, and did so via learning embeddings as part of the prediction process.
The neural language model was extended in various ways over the years, perhaps
most importantly in the form of the RNN language model of Mikolov et al. (2010)
and Mikolov et al. (2011). The RNN language model was perhaps the ﬁrst neural
model that was accurate enough to surpass the performance of a traditional 5-gram
language model.
Soon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed
to simply the hidden layer of these neural net language models to create pretrained
word2vec word embeddings.
The static embedding models like LSA and word2vec instantiated a particular
model of pretraining: a representation was trained on a pretraining dataset, and then
the representations could be used in further tasks. The ‘Dai and Le (2015) and
(Peters et al., 2018) reframed this idea by proposing models that were pretrained
using a language model objective, and then the identical model could be either frozen
and directly applied for language modeling or further ﬁnetuned still using a language
model objective. For example ELMo used a biLSTM self-supervised on a large
pretrained dataset using a language model objective, then ﬁnetuned on a domain-
speciﬁc dataset, and then froze the weights and added task-speciﬁc heads.
Transformers were ﬁrst applied as encoder-decoders (Vaswani et al., 2017) and
then to masked language modeling (Devlin et al., 2019) (as we’ll see in Chapter 13
and Chapter 11). Radford et al. (2019) then showed that the transformer-based au-
toregressive language model GPT2 could perform zero-shot on many NLP tasks like
summarization and question answering.
The technology used for transformer-based language models can also be applied
to other domains and tasks, like vision, speech, and genetics. the term foundation
model is sometimes used as a more general term for this use of large language
foundation
model
model technology across domains and areas, when the elements we are computing
over are not necessarily words. Bommasani et al. (2021) is a broad survey that
20
CHAPTER 10
•
LARGE LANGUAGE MODELS
sketches the opportunities and risks of foundation models, with special attention to
large language models.
Bibliographical and Historical Notes
21
Bellegarda, J. R. 1997. A latent semantic analysis framework
for large-span language modeling. EUROSPEECH.
Bengio, Y., R. Ducharme, and P. Vincent. 2000. A neural
probabilistic language model. NeurIPS.
Bengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. JMLR, 3:1137–
1155.
Bengio, Y., H. Schwenk, J.-S. Sen´ecal, F. Morin, and J.-L.
Gauvain. 2006. Neural probabilistic language models. In
Innovations in Machine Learning, 137–186. Springer.
Bommasani, R., D. A. Hudson, E. Adeli, R. Altman,
S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosse-
lut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card,
R. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel,
J. Davis, D. Demszky, C. Donahue, M. Doumbouya,
E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh,
L. Fei-Fei, C. Finn, T. Gale, L. E. Gillespie, K. Goel,
N. D. Goodman, S. Grossman, N. Guha, T. Hashimoto,
P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu,
J. Huang, T. F. Icard, S. Jain, D. Jurafsky, P. Kalluri,
S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W.
Koh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Ku-
mar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Lev-
ent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning,
S. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair,
A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C.
Niebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr,
I. Papadimitriou, J. S. Park, C. Piech, E. Portelance,
C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong,
Y. H. Roohani, C. Ruiz, J. Ryan, C. R’e, D. Sadigh,
S. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan,
A. Tamkin, R. Taori, A. W. Thomas, F. Tram`er, R. E.
Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Ya-
sunaga, J. You, M. A. Zaharia, M. Zhang, T. Zhang,
X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang.
2021. On the opportunities and risks of foundation mod-
els. ArXiv.
Brown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan,
P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,
C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,
S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
A. Radford, I. Sutskever, and D. Amodei. 2020. Language
models are few-shot learners. NeurIPS, volume 33.
Carlini, N., F. Tramer, E. Wallace, M. Jagielski, A. Herbert-
Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Er-
lingsson, et al. 2021. Extracting training data from large
language models.
30th USENIX Security Symposium
(USENIX Security 21).
Cheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per-
sonas: Using natural language prompts to measure stereo-
types in language models. ACL.
Coccaro, N. and D. Jurafsky. 1998. Towards better integra-
tion of semantic predictors in statistical language model-
ing. ICSLP.
Dai, A. M. and Q. V. Le. 2015. Semi-supervised sequence
learning. NeurIPS.
Deerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-
man, T. K. Landauer, K. E. Lochbaum, and L. Streeter.
1988. Computer information retrieval using latent seman-
tic structure: US Patent 4,839,853.
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019.
BERT: Pre-training of deep bidirectional transformers for
language understanding. NAACL HLT.
Dodge, J., S. Gururangan, D. Card, R. Schwartz, and N. A.
Smith. 2019. Show your work: Improved reporting of
experimental results. EMNLP.
Dodge, J., M. Sap, A. Marasovi´c, W. Agnew, G. Ilharco,
D. Groeneveld, M. Mitchell, and M. Gardner. 2021. Doc-
umenting large webtext corpora: A case study on the
colossal clean crawled corpus. EMNLP.
Ethayarajh, K. and D. Jurafsky. 2020. Utility is in the eye of
the user: A critique of NLP leaderboards. EMNLP.
Gao, L., T. Hoppe, A. Thite, S. Biderman, C. Foster,
N. Nabeshima, S. Black, J. Phang, S. Presser, L. Golding,
H. He, and C. Leahy. 2020. The Pile: An 800GB dataset
of diverse text for language modeling. ArXiv preprint.
Gehman, S., S. Gururangan, M. Sap, Y. Choi, and N. A.
Smith. 2020.
RealToxicityPrompts:
Evaluating neu-
ral toxic degeneration in language models. Findings of
EMNLP.
Gururangan, S., A. Marasovi´c, S. Swayamdipta, K. Lo,
I. Beltagy, D. Downey, and N. A. Smith. 2020. Don’t
stop pretraining: Adapt language models to domains and
tasks. ACL.
Hashimoto, T., M. Srivastava, H. Namkoong, and P. Liang.
2018.
Fairness without demographics in repeated loss
minimization. ICML.
Heaﬁeld, K. 2011.
KenLM: Faster and smaller language
model queries. Workshop on Statistical Machine Trans-
lation.
Henderson, P., J. Hu, J. Romoff, E. Brunskill, D. Jurafsky,
and J. Pineau. 2020. Towards the systematic reporting
of the energy and carbon footprints of machine learning.
Journal of Machine Learning Research, 21(248):1–43.
Henderson, P., X. Li, D. Jurafsky, T. Hashimoto, M. A. Lem-
ley, and P. Liang. 2023. Foundation models and fair use.
JMLR, 24(400):1–79.
Henderson, P., K. Sinha, N. Angelard-Gontier, N. R. Ke,
G. Fried, R. Lowe, and J. Pineau. 2017. Ethical chal-
lenges in data-driven dialogue systems. AAAI/ACM AI
Ethics and Society Conference.
Hermann, K. M., T. Koˇcisk´y, E. Grefenstette, L. Espeholt,
W. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching
machines to read and comprehend. NeurIPS.
Holtzman, A., J. Buys, L. Du, M. Forbes, and Y. Choi. 2020.
The curious case of neural text degeneration. ICLR.
Jelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a
linguistic statistical decoder for the recognition of contin-
uous speech. IEEE Transactions on Information Theory,
IT-21(3):250–256.
Kaplan, J., S. McCandlish, T. Henighan, T. B. Brown,
B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and
D. Amodei. 2020. Scaling laws for neural language mod-
els. ArXiv preprint.
Kiela, D., M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu,
B. Vidgen, G. Prasad, A. Singh, P. Ringshia, et al. 2021.
Dynabench: Rethinking benchmarking in nlp. NAACL
HLT.
22
Chapter 10
•
Large Language Models
Landauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner.
1997. How well can passage meaning be derived with-
out using word order? A comparison of Latent Semantic
Analysis and humans. COGSCI.
Liang, P., R. Bommasani, T. Lee, D. Tsipras, D. Soylu,
M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Ku-
mar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cos-
grove, C. D. Manning, C. R´e, D. Acosta-Navas, D. A.
Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong,
H. Ren, H. Yao, J. Wang, K. Santhanam, L. Orr, L. Zheng,
M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha, N. Chat-
terji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M.
Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard,
T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai,
Y. Zhang, and Y. Koreeda. 2023. Holistic evaluation of
language models. Transactions on Machine Learning Re-
search.
Llama Team. 2024. The llama 3 herd of models.
Longpre, S., R. Mahari, A. Lee, C. Lund, H. Oderinwale,
W. Brannon, N. Saxena, N. Obeng-Marnu, T. South,
C. Hunter, et al. 2024a. Consent in crisis: The rapid de-
cline of the ai data commons. ArXiv preprint.
Longpre, S., G. Yauney, E. Reif, K. Lee, A. Roberts,
B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno, and
D. Ippolito. 2024b. A pretrainer’s guide to training data:
Measuring the effects of data age, domain coverage, qual-
ity, & toxicity. NAACL HLT.
McGufﬁe, K. and A. Newhouse. 2020. The radicalization
risks of GPT-3 and advanced neural language models.
ArXiv preprint arXiv:2009.06807.
Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Ef-
ﬁcient estimation of word representations in vector space.
ICLR 2013.
Mikolov, T., M. Karaﬁ´at, L. Burget, J. ˇCernock`y, and
S. Khudanpur. 2010. Recurrent neural network based lan-
guage model. INTERSPEECH.
Mikolov, T., S. Kombrink, L. Burget, J. H. ˇCernock`y, and
S. Khudanpur. 2011. Extensions of recurrent neural net-
work language model. ICASSP.
Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. 2013b. Distributed representations of words and
phrases and their compositionality. NeurIPS.
Nadeem, M., A. Bethke, and S. Reddy. 2021.
StereoSet:
Measuring stereotypical bias in pretrained language mod-
els. ACL.
Nallapati, R., B. Zhou, C. dos Santos, C¸ . Gulc¸ehre, and
B. Xiang. 2016.
Abstractive text summarization using
sequence-to-sequence RNNs and beyond. CoNLL.
Olteanu, A., F. Diaz, and G. Kazai. 2020. When are search
completion suggestions problematic? CSCW.
Parrish, A., A. Chen, N. Nangia, V. Padmakumar, J. Phang,
J. Thompson, P. M. Htut, and S. Bowman. 2022. BBQ: A
hand-built bias benchmark for question answering. Find-
ings of ACL 2022.
Peters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark,
K. Lee, and L. Zettlemoyer. 2018. Deep contextualized
word representations. NAACL HLT.
Radford, A., J. Wu, R. Child, D. Luan, D. Amodei, and
I. Sutskever. 2019. Language models are unsupervised
multitask learners. OpenAI tech report.
Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,
M. Matena, Y. Zhou, W. Li, and P. J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. JMLR, 21(140):1–67.
Rawls, J. 2001. Justice as fairness: A restatement. Harvard
University Press.
Rosenfeld, R. 1992.
Adaptive Statistical Language Mod-
eling: A Maximum Entropy Approach.
Ph.D. thesis,
Carnegie Mellon University.
Rosenfeld, R. 1996. A maximum entropy approach to adap-
tive statistical language modeling. Computer Speech and
Language, 10:187–228.
Sagawa, S., P. W. Koh, T. B. Hashimoto, and P. Liang. 2020.
Distributionally robust neural networks for group shifts:
On the importance of regularization for worst-case gener-
alization. ICLR.
Sheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019.
The woman worked as a babysitter: On biases in language
generation. EMNLP.
Soldaini, L., R. Kinney, A. Bhagia, D. Schwenk, D. Atkin-
son, R. Authur, B. Bogin, K. Chandu, J. Dumas,
Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy,
X. Lyu,
N. Lambert,
I. Magnusson,
J. Morrison,
N. Muennighoff, A. Naik, C. Nam, M. E. Peters,
A. Ravichander, K. Richardson, Z. Shen, E. Strubell,
N. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A.
Smith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge,
and K. Lo. 2024. Dolma: An open corpus of three trillion
tokens for language model pretraining research. ArXiv
preprint.
Stolcke, A. 2002. SRILM – an extensible language modeling
toolkit. ICSLP.
Strubell, E., A. Ganesh, and A. McCallum. 2019. Energy
and policy considerations for deep learning in NLP. ACL.
Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Atten-
tion is all you need. NeurIPS.
Xu, A., E. Pathak, E. Wallace, S. Gururangan, M. Sap,
and D. Klein. 2021. Detoxifying language models risks
marginalizing minority voices. NAACL HLT.
